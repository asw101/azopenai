/*
Package completions provides a client for the Completions API.

This allows you to generate text responses to textual prompts a user provides. The simpliest
way to create a Client is by using the azopenai.Client.Completions() method.

Using this API is simple:

	completions := azopenai.Client.Completions()
	resp, err := completions.Call([]string{"The capital of Tennessee is"})
	if err != nil {
		// handle error
	}
	fmt.Println(resp.Completions[0].Text)
*/
package completions

import (
	"context"
	"strings"
	"sync/atomic"

	"github.com/element-of-surprise/azopenai/rest"
	"github.com/element-of-surprise/azopenai/rest/messages"
)

type Client struct {
	rest *rest.Client

	CallParams atomic.Pointer[CallParams]
}

// New creates a new instance of the Client type from the rest.Client. This is generally
// not used directly, but is used by the azopenai.Client.
func New(rest *rest.Client) *Client {
	return &Client{
		rest: rest,
	}
}

var defaults = CallParams{
	MaxTokens:   16,
	Temperature: 1,
	TopP:        1,
	N:           1,
	Stop:        []string{`<|endoftext|>`},
}

type CallParams struct {
	// MaxTokens is the token count of your prompt. This cannot exceed the model's context length.
	// Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.
	MaxTokens int `json:"max_tokens"`
	// Temperature is the sampling temperature to use. Higher values means the model will take more risks.
	// Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
	// It is generally recommend altering this or TopP but not both.
	Temperature float64 `json:"temperature"`
	// TopP is an alternative to sampling with temperature, called nucleus sampling.
	// This is where the model considers the results of the tokens with TopP probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// It is generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p"`
	// LogitBias is the likelihood of specified tokens appearing in the completion.
	// This maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100.
	// You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	// As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated.
	LogitBias map[string]float64 `json:"logit_bias,omitempty"`
	// User is a unique identifier representing your end-user, which can help monitoring and detecting abuse.
	User string `json:"user,omitempty"`
	// N is the number of completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
	// Note: Because this parameter generates many completions, it can quickly consume your token quota.
	// Use carefully and ensure that you have reasonable settings for MaxTokens and stop.
	N int `json:"n"`
	// Stream indicates whether to stream back partial progress. If set, tokens will be sent as data-only server-sent
	// events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"`
	// Logprobs include the log probabilities on the logprobs most likely tokens, as well the chosen tokens.
	// For example, if logprobs is 5, the API will return a list of the 5 most likely tokens.
	// The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
	// Minimum of 0 and maximum of 5 allowed.
	Logprobs int `json:"logprobs"`
	// Model is the ID of the model to use. You can use the ModelsList operation to see all of your available models,
	// or see ModelsGet overview for descriptions of them
	Model string `json:"model"`
	// Suffix is the suffix that comes after a completion of inserted text.
	Suffix string `json:"suffix,omitempty"`
	// Echo indicates if the response should echo back the prompt in addition to the completion.
	Echo bool `json:"echo,omitempty"`
	// Stop  provides up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
	Stop []string `json:"stop,omitempty"`
}

// Defaults returns a CallParams with default values set. This should be called before
// setting any values as it will override any values that are set.
func (c CallParams) Defaults() CallParams {
	c.MaxTokens = defaults.MaxTokens
	c.Temperature = defaults.Temperature
	c.TopP = defaults.TopP
	c.LogitBias = defaults.LogitBias
	c.User = defaults.User
	c.N = defaults.N
	c.Stream = defaults.Stream
	c.Logprobs = defaults.Logprobs
	c.Model = defaults.Model
	c.Suffix = defaults.Suffix
	c.Echo = defaults.Echo
	c.Stop = defaults.Stop
	return c
}

func (c CallParams) toPromptRequest() messages.PromptRequest {
	return messages.PromptRequest{
		MaxTokens:   c.MaxTokens,
		Temperature: c.Temperature,
		TopP:        c.TopP,
		LogitBias:   c.LogitBias,
		User:        c.User,
		N:           c.N,
		Stream:      c.Stream,
		Logprobs:    c.Logprobs,
		Model:       c.Model,
		Suffix:      c.Suffix,
		Echo:        c.Echo,
		Stop:        c.Stop,
	}
}

// SetParams sets the CallParams for the client. This will be used for all calls unless
// overridden by a CallOption.
func (c *Client) SetParams(params CallParams) {
	c.CallParams.Store(&params)
}

type Completions struct {
	// Text is the completion texts from the server.
	Text []string

	// RestReq is the raw request sent to the REST API. This is only provided if a specific
	// CallOption is used.
	RestReq messages.PromptRequest
	// RestResp is the raw response from the REST API. This is only provided if a specific
	// CallOption is used.
	RestResp messages.PromptResponse
}

type callOptions struct {
	CallParams    CallParams
	setCallParams bool

	RestReq  bool
	RestResp bool
}

// CallOption is an optional argument for the Call method.
type CallOption func(options *callOptions) error

// WithCallParams sets the CallParams for the call. If not set, the call params set for
// the client will be used. If those weren't set, the default call options are used.
func WithCallParams(params CallParams) CallOption {
	return func(o *callOptions) error {
		o.CallParams = params
		o.setCallParams = true
		return nil
	}
}

// WithRest sets whether to return the raw REST request and response. This is useful for
// debugging purposes.
func WithRest(req, resp bool) CallOption {
	return func(o *callOptions) error {
		o.RestReq = req
		o.RestResp = resp
		return nil
	}
}

// Call makes a call to the Completions API endpoint and returns the completions for the prompts.
func (c *Client) Call(ctx context.Context, prompts []string, options ...CallOption) (Completions, error) {
	callOptions := callOptions{}
	for _, o := range options {
		if err := o(&callOptions); err != nil {
			return Completions{}, err
		}
	}
	if !callOptions.setCallParams {
		callOptions.CallParams = defaults
		p := c.CallParams.Load()
		if p != nil {
			callOptions.CallParams = *p
		}
	}

	// Remove any leading or trailing spaces as the OpenAI API doesn't like them.
	for i := 0; i < len(prompts); i++ {
		prompts[i] = strings.TrimSpace(prompts[i])
	}

	req := callOptions.CallParams.toPromptRequest()
	req.Prompt = prompts

	resp, err := c.rest.Completions(ctx, req)
	if err != nil {
		return Completions{}, err
	}

	compl := Completions{}
	if callOptions.RestReq {
		compl.RestReq = req
	}
	if callOptions.RestResp {
		compl.RestResp = resp
	}
	for _, choice := range resp.Choices {
		compl.Text = append(compl.Text, choice.Text)
	}
	return compl, nil
}

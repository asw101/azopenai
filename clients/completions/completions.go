/*
Package completions provides a client for the Completions API.

This allows you to generate text responses to textual prompts a user provides. The simplest
way to create a Client is by using the azopenai.Client.Completions() method.

Using this API is simple:

	ctx := context.Background()
	completions := client.Completions()
	resp, err := completions.Call(ctx, []string{"The capital of California is"})
	if err != nil {
		return err
	}
	fmt.Println(resp.Text[0])

You can also set the default parameters for the client:

	completionsClient := client.Completions()

	// This creates a new instance of CallParams with the default values.
	// We then modify then and set them on the client. They will be used on
	// every call unless you override them on a specific call.
	params := completions.CallParams{}.Defaults()
	params.MaxTokens = 32
	params.Temperature = 0.5
	completionsClient.SetParams(params)

	ctx := context.Background()
	resp, err := completionsClient.Call(ctx, []string{"The capital of California is"})
	if err != nil {
		return err
	}
	fmt.Println(resp.Text[0])

You can also override the parameters on a per-call basis:

	ctx := context.Background()
	resp, err := completionsClient.Call(ctx, []string{"The capital of California is"}, completions.WithCallParams(params))
	if err != nil {
		return err
	}
	fmt.Println(resp.Text[0])
*/
package completions

import (
	"context"
	"strings"
	"sync/atomic"

	"github.com/element-of-surprise/azopenai/rest"
	"github.com/element-of-surprise/azopenai/rest/messages/completions"
)

type Client struct {
	deploymentID string
	rest         *rest.Client

	callParams atomic.Pointer[CallParams]
}

// New creates a new instance of the Client type from the rest.Client. This is generally
// not used directly, but is used by the azopenai.Client.
func New(deploymentID string, rest *rest.Client) *Client {
	return &Client{
		deploymentID: deploymentID,
		rest:         rest,
	}
}

var defaults = CallParams{
	MaxTokens:   16,
	Temperature: 1,
	TopP:        1,
	N:           1,
	Stop:        []string{`<|endoftext|>`},
}

// CallParams are the parameters used on each call to the completions service. These
// are all optional fields. You can set this on the client and override it on a per-call
// basis.
type CallParams struct {
	// LogitBias is the likelihood of specified tokens appearing in the completion.
	// This maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100.
	// You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	// As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated.
	LogitBias map[string]float64 `json:"logit_bias,omitempty"`
	// User is a unique identifier representing your end-user, which can help monitoring and detecting abuse.
	User string `json:"user,omitempty"`
	// Model is the ID of the model to use. You can use the ModelsList operation to see all of your available models,
	// or see ModelsGet overview for descriptions of them
	Model string `json:"model"`
	// Suffix is the suffix that comes after a completion of inserted text.
	Suffix string `json:"suffix,omitempty"`
	// Stop  provides up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
	Stop []string `json:"stop,omitempty"`
	// MaxTokens is the token count of your prompt. This cannot exceed the model's context length.
	// Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.
	MaxTokens int `json:"max_tokens"`
	// Temperature is the sampling temperature to use. Higher values means the model will take more risks.
	// Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
	// It is generally recommend altering this or TopP but not both.
	Temperature float64 `json:"temperature"`
	// TopP is an alternative to sampling with temperature, called nucleus sampling.
	// This is where the model considers the results of the tokens with TopP probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// It is generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p"`
	// N is the number of completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
	// Note: Because this parameter generates many completions, it can quickly consume your token quota.
	// Use carefully and ensure that you have reasonable settings for MaxTokens and stop.
	N int `json:"n"`
	// Logprobs include the log probabilities on the logprobs most likely tokens, as well the chosen tokens.
	// For example, if logprobs is 5, the API will return a list of the 5 most likely tokens.
	// The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
	// Minimum of 0 and maximum of 5 allowed.
	Logprobs int `json:"logprobs"`
	// Stream indicates whether to stream back partial progress. If set, tokens will be sent as data-only server-sent
	// events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"`
	// Echo indicates if the response should echo back the prompt in addition to the completion.
	Echo bool `json:"echo,omitempty"`
}

// Defaults returns a CallParams with default values set. This should be called before
// setting any values as it will override any values that are set.
func (c CallParams) Defaults() CallParams {
	c.MaxTokens = defaults.MaxTokens
	c.Temperature = defaults.Temperature
	c.TopP = defaults.TopP
	c.LogitBias = defaults.LogitBias
	c.User = defaults.User
	c.N = defaults.N
	c.Stream = defaults.Stream
	c.Logprobs = defaults.Logprobs
	c.Model = defaults.Model
	c.Suffix = defaults.Suffix
	c.Echo = defaults.Echo
	c.Stop = defaults.Stop
	return c
}

func (c CallParams) toPromptRequest() completions.Req {
	return completions.Req{
		MaxTokens:   c.MaxTokens,
		Temperature: c.Temperature,
		TopP:        c.TopP,
		LogitBias:   c.LogitBias,
		User:        c.User,
		N:           c.N,
		Stream:      c.Stream,
		Logprobs:    c.Logprobs,
		Model:       c.Model,
		Suffix:      c.Suffix,
		Echo:        c.Echo,
		Stop:        c.Stop,
	}
}

// SetParams sets the CallParams for the client. This will be used for all calls unless
// overridden by a CallOption.
func (c *Client) SetParams(params CallParams) {
	c.callParams.Store(&params)
}

// Completions are the completions returned from the API.
type Completions struct {
	// Text is the completion texts from the server.
	Text []string

	// RestReq is the raw request sent to the REST API. This is only provided if a specific
	// CallOption is used.
	RestReq completions.Req
	// RestResp is the raw response from the REST API. This is only provided if a specific
	// CallOption is used.
	RestResp completions.Resp
}

type callOptions struct {
	CallParams    CallParams
	setCallParams bool
	DeploymentID  string

	RestReq  bool
	RestResp bool
}

// CallOption is an optional argument for the Call method.
type CallOption func(options *callOptions) error

// WithCallParams sets the CallParams for the call. If not set, the call params set for
// the client will be used. If those weren't set, the default call options are used.
func WithCallParams(params CallParams) CallOption {
	return func(o *callOptions) error {
		o.CallParams = params
		o.setCallParams = true
		return nil
	}
}

// WithDeploymentID sets the deployment ID to use for the call. If not set, the deploymentID
// set on the client will be used.
func WithDeploymentID(deploymentID string) CallOption {
	return func(o *callOptions) error {
		o.DeploymentID = deploymentID
		return nil
	}
}

// WithRest sets whether to return the raw REST request and response. This is useful for
// debugging purposes.
func WithRest(req, resp bool) CallOption {
	return func(o *callOptions) error {
		o.RestReq = req
		o.RestResp = resp
		return nil
	}
}

// Call makes a call to the Completions API endpoint and returns the completions for the prompts.
func (c *Client) Call(ctx context.Context, prompts []string, options ...CallOption) (Completions, error) {
	req, callOptions, err := c.prep(prompts, options...)
	if err != nil {
		return Completions{}, err
	}

	deploymentID := c.deploymentID
	if callOptions.DeploymentID != "" {
		deploymentID = callOptions.DeploymentID
	}

	resp, err := c.rest.Completions(ctx, deploymentID, req)
	if err != nil {
		return Completions{}, err
	}

	compl := Completions{}
	if callOptions.RestReq {
		compl.RestReq = req
	}
	if callOptions.RestResp {
		compl.RestResp = resp
	}
	for _, choice := range resp.Choices {
		compl.Text = append(compl.Text, choice.Text)
	}
	return compl, nil
}

// StreamData is used to receive data from the stream.
type StreamData struct {
	// Err is an error related to the stream. The stream is terminated after this.
	Err error
	// Data is data sent by the stream.
	Data Completions
}

// Stream makes a call to the Completions API endpoint and returns a channel that will return
// the completions for the prompt as they are received. Unlike the Call() method, each return
// value will be part of a single completion.
func (c *Client) Stream(ctx context.Context, prompts string, options ...CallOption) chan StreamData {
	ch := make(chan StreamData, 1)

	req, callOptions, err := c.prep([]string{prompts}, options...)
	if err != nil {
		ch <- StreamData{Err: err}
		return ch
	}

	deploymentID := c.deploymentID
	if callOptions.DeploymentID != "" {
		deploymentID = callOptions.DeploymentID
	}

	go func() {
		defer close(ch)

		responses := c.rest.CompletionsStream(ctx, deploymentID, req)
		if err != nil {
			ch <- StreamData{Err: err}
			return
		}

		for resp := range responses {
			if resp.Err != nil {
				ch <- StreamData{Err: resp.Err}
				return
			}

			compl := Completions{}
			if callOptions.RestReq {
				compl.RestReq = req
			}
			if callOptions.RestResp {
				compl.RestResp = resp.Data
			}
			for _, choice := range resp.Data.Choices {
				compl.Text = append(compl.Text, choice.Text)
			}
			ch <- StreamData{Data: compl}
		}
	}()

	return ch
}

func (c *Client) prep(prompts []string, options ...CallOption) (completions.Req, callOptions, error) {
	callOptions := callOptions{}
	for _, o := range options {
		if err := o(&callOptions); err != nil {
			return completions.Req{}, callOptions, err
		}
	}
	if !callOptions.setCallParams {
		callOptions.CallParams = defaults
		p := c.callParams.Load()
		if p != nil {
			callOptions.CallParams = *p
		}
	}

	// Remove any leading or trailing spaces as the OpenAI API doesn't like them.
	for i := 0; i < len(prompts); i++ {
		prompts[i] = strings.TrimSpace(prompts[i])
	}

	req := callOptions.CallParams.toPromptRequest()
	req.Prompt = prompts
	return req, callOptions, nil
}

package completions

import (
	"context"
	"sync/atomic"

	"github.com/element-of-surprise/azopenai/rest"
	"github.com/element-of-surprise/azopenai/rest/messages/chat"
)

// Client provides access to the Chat API. Chat allows you to generate text in response
// to text that is provided.
type Client struct {
	rest *rest.Client

	CallParams atomic.Pointer[CallParams]
}

// New creates a new instance of the Client type from the rest.Client. This is generally
// not used directly, but is used by the azopenai.Client.
func New(rest *rest.Client) *Client {
	return &Client{
		rest: rest,
	}
}

var defaults = CallParams{
	MaxTokens:   4096,
	Temperature: 1,
	TopP:        1,
	N:           1,
}

// CallParams are the parameters used on each call to the chat service. These
// are all optional fields. You can set this on the client and override it on a per-call
// basis.
type CallParams struct {
	// Stop provides up to 4 sequences where the API will stop generating further tokens.
	Stop []string

	// LogitBias is the likelihood of specified tokens appearing in the completion.
	// This maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100.
	// You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	// As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated.
	LogitBias map[string]float64

	// User is a unique identifier representing your end-user, which can help monitoring and detecting abuse.
	User string

	// N is the number of completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
	// Note: Because this parameter generates many completions, it can quickly consume your token quota.
	// Use carefully and ensure that you have reasonable settings for MaxTokens and stop.
	N int

	// MaxTokens is the token count of your prompt. This cannot exceed the model's context length.
	// Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.
	MaxTokens int

	// Temperature is the sampling temperature to use. Higher values means the model will take more risks.
	// Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
	// It is generally recommend altering this or TopP but not both.
	Temperature float64

	// TopP is an alternative to sampling with temperature, called nucleus sampling.
	// This is where the model considers the results of the tokens with TopP probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// It is generally recommend altering this or temperature but not both.
	TopP float64

	// PresencePenalty is a float64 between -2.0 and 2.0. Positive values penalize new tokens based on
	// whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty float64

	// FrequencyPenalty is a float64 between -2.0 and 2.0. Positive values penalize new tokens based on their
	// existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty float64
}

// Defaults returns a CallParams with default values set. This should be called before
// setting any values as it may override values that are set.
func (c CallParams) Defaults() CallParams {
	c.MaxTokens = defaults.MaxTokens
	c.Temperature = defaults.Temperature
	c.TopP = defaults.TopP
	c.User = defaults.User
	c.N = defaults.N
	return c
}

func (c CallParams) toPromptRequest() chat.Req {
	return chat.Req{
		MaxTokens:   c.MaxTokens,
		Temperature: c.Temperature,
		TopP:        c.TopP,
		LogitBias:   c.LogitBias,
		User:        c.User,
		N:           c.N,
		Stop:        c.Stop,
	}
}

// SetParams sets the CallParams for the client. This will be used for all calls unless
// overridden by a CallOption.
func (c *Client) SetParams(params CallParams) {
	c.CallParams.Store(&params)
}

// Chats returns the response texts for the text sent.
type Chats struct {
	// Text is the response texts from the server.
	Text []string

	// RestReq is the raw request sent to the REST API. This is only provided if a specific
	// CallOption is used.
	RestReq chat.Req
	// RestResp is the raw response from the REST API. This is only provided if a specific
	// CallOption is used.
	RestResp chat.Resp
}

type callOptions struct {
	CallParams    CallParams
	setCallParams bool

	RestReq  bool
	RestResp bool
}

// CallOption is an optional argument for the Call method.
type CallOption func(options *callOptions) error

// WithCallParams sets the CallParams for the call. If not set, the call params set for
// the client will be used. If those weren't set, the default call options are used.
func WithCallParams(params CallParams) CallOption {
	return func(o *callOptions) error {
		o.CallParams = params
		o.setCallParams = true
		return nil
	}
}

// WithRest sets whether to return the raw REST request and response. This is useful for
// debugging purposes.
func WithRest(req, resp bool) CallOption {
	return func(o *callOptions) error {
		o.RestReq = req
		o.RestResp = resp
		return nil
	}
}

// Role is a the type of role of the author of a message.
type Role string

const (
	// UnknownRole is the default value for Role, indicating that the role was not set.
	UnknownRole Role = ""
	// User is a user in a chat.
	User Role = "user"
	// System is a system message.
	System Role = "system"
	// Assistant is an assistant message.
	Assistant Role = "assistant"
)

// SendMsg is a message to send to the chat API.
type SendMsg struct {
	// Role of the author of this message.
	Role Role

	// Contents of the message.
	Content string

	// Name of the user in chat.
	Name string
}

func (s SendMsg) toSendMsg() chat.SendMsg {
	return chat.SendMsg{
		Role:    chat.Role(s.Role),
		Content: s.Content,
		Name:    s.Name,
	}
}

// Call makes a call to the Chat API endpoint and returns the chat results.
func (c *Client) Call(ctx context.Context, messages []SendMsg, options ...CallOption) (Chats, error) {
	callOptions := callOptions{}
	for _, o := range options {
		if err := o(&callOptions); err != nil {
			return Chats{}, err
		}
	}
	if !callOptions.setCallParams {
		callOptions.CallParams = defaults
		p := c.CallParams.Load()
		if p != nil {
			callOptions.CallParams = *p
		}
	}

	req := callOptions.CallParams.toPromptRequest()

	for _, m := range messages {
		req.Messages = append(req.Messages, m.toSendMsg())
	}

	resp, err := c.rest.Chat(ctx, req)
	if err != nil {
		return Chats{}, err
	}

	chats := Chats{}
	if callOptions.RestReq {
		chats.RestReq = req
	}
	if callOptions.RestResp {
		chats.RestResp = resp
	}

	for _, choice := range resp.Choices {
		chats.Text = append(chats.Text, choice.Message.Content)
	}
	return chats, nil
}

package messages

import (
	"fmt"
	"strconv"
	"strings"
	"time"
)

// PromptRequest is used to create a completion for the provided prompts, parameters and chosen model.
type PromptRequest struct {
	// Prompt provides the prompts to generate completions for, encoded as a string or array of strings.
	// Note that <|endoftext|> is the document separator that the model sees during training,
	// so if a prompt is not specified the model will generate as if from the beginning of a new document.
	// Maximum allowed size of string list is 2048.
	Prompt []string `json:"prompt,omitempty"`
	// MaxTokens is the token count of your prompt. This cannot exceed the model's context length.
	// Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.
	MaxTokens int `json:"max_tokens"`
	// Temperature is the sampling temperature to use. Higher values means the model will take more risks.
	// Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.
	// It is generally recommend altering this or TopP but not both.
	Temperature float64 `json:"temperature"`
	// TopP is an alternative to sampling with temperature, called nucleus sampling.
	// This is where the model considers the results of the tokens with TopP probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// It is generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p"`
	// LogitBias is the likelihood of specified tokens appearing in the completion.
	// This maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100.
	// You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs.
	// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	// As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated.
	LogitBias map[string]float64 `json:"logit_bias,omitempty"`
	// User is a unique identifier representing your end-user, which can help monitoring and detecting abuse.
	User string `json:"user,omitempty"`
	// N is the number of completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed.
	// Note: Because this parameter generates many completions, it can quickly consume your token quota.
	// Use carefully and ensure that you have reasonable settings for MaxTokens and stop.
	N int `json:"n"`
	// Stream indicates whether to stream back partial progress. If set, tokens will be sent as data-only server-sent
	// events as they become available, with the stream terminated by a data: [DONE] message.
	Stream bool `json:"stream,omitempty"`
	// Logprobs include the log probabilities on the logprobs most likely tokens, as well the chosen tokens.
	// For example, if logprobs is 5, the API will return a list of the 5 most likely tokens.
	// The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
	// Minimum of 0 and maximum of 5 allowed.
	Logprobs int `json:"logprobs"`
	// Model is the ID of the model to use. You can use the ModelsList operation to see all of your available models,
	// or see ModelsGet overview for descriptions of them
	Model string `json:"model"`
	// Suffix is the suffix that comes after a completion of inserted text.
	Suffix string `json:"suffix,omitempty"`
	// Echo indicates if the response should echo back the prompt in addition to the completion.
	Echo bool `json:"echo,omitempty"`
	// Stop  provides up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
	Stop []string `json:"stop,omitempty"`
}

// Defaults sets all the default values for fields if the field is set to the zero value of the type. This will overwrite fields that have valid zero values
// and defaults that are not the zero value. For example, Temperature has a valid value of 0 and a default of 1. Calling this would set
// Temperature to 1 if the value was 0. It is suggested to use this before setting fields.
func (p *PromptRequest) Defaults() {
	if p.MaxTokens == 0 {
		p.MaxTokens = 16
	}
	if p.Temperature == 0 {
		p.Temperature = 1
	}
	if p.TopP == 0 {
		p.TopP = 1
	}
	if p.N == 0 {
		p.N = 1
	}
	if p.Stop == nil {
		p.Stop = []string{`<|endoftext|>`}
	}
}

func (p *PromptRequest) validate() error {
	if len(p.Prompt) > 2048 {
		return fmt.Errorf("cannot have a prompt list with more than 2048 entries")
	}
	if p.MaxTokens < 0 || p.MaxTokens > 4096 {
		return fmt.Errorf("cannot set MaxTokens < 0 or > 4096")
	}
	if p.N < 1 || p.N > 128 {
		return fmt.Errorf("cannot set N < 1 or > 128")
	}
	if p.Logprobs < 0 || p.Logprobs > 5 {
		return fmt.Errorf("cannot set Logprobs < 0 or > 5")
	}
	if len(p.Stop) > 4 {
		return fmt.Errorf("Stop cannot have more than 4 entries")
	}
	return nil
}

type PromptResponse struct {
	Created UnixTime  `json:"created"`
	ID      string    `json:"id"`
	Object  string    `json:"object"`
	Model   string    `json:"model"`
	Choices []Choices `json:"choices"`
}

type Choices struct {
	Text         string   `json:"text"`
	FinishReason string   `json:"finish_reason"`
	Logprobs     LogProbs `json:"logprobs"`
	Index        int      `json:"index"`
}

type LogProbs struct {
	Tokens        []string             `json:"tokens"`
	TokenLogProbs []float64            `json:"token_logprobs"`
	TopLogProbs   []map[string]float64 `json:"top_logprobs"`
	TextOffset    []int                `json:"text_offset"`
}

// UnixTime is a wrapper around time.Time that allows for marshaling and unmarshaling of
// unix timestamps.
type UnixTime struct {
	// Time is the underlying time.Time value.
	Time time.Time
}

// UnmarshalJSON unmarshals a unix timestamp into a time.Time.
func (u *UnixTime) UnmarshalJSON(b []byte) error {
	s := strings.Trim(string(b), "\"")
	if s == "null" {
		u.Time = time.Time{}
		return nil
	}
	i, err := strconv.Atoi(s)
	if err != nil {
		return fmt.Errorf("expected epoch time, got %q: %w", s, err)
	}
	u.Time = time.Unix(int64(i), 0)
	return nil
}

// MarshalJSON marshals a time.Time into a unix timestamp.
func (u *UnixTime) MarshalJSON() ([]byte, error) {
	if u.Time.IsZero() {
		return nil, nil
	}
	return []byte(fmt.Sprintf("%d", u.Time.Unix())), nil
}
